---
title: "Writeup for Final Project "
author: "Paul Motter"
date: "May 12, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Thanks to my Data Science Professor Hector Corida Bravo and his [notes](http://www.hcbravo.org/IntroDataSci/bookdown-notes/). These helped me immensily for this project. 

##Introduction

A popular source of argument among friends, enemies, frenemies, and family members (which could be in any of the previous categories) is the state of gun ownership and gun violence in the United States today. I know I have (cordially of course) discussed this issue a fair bit, especially after the Vegas and Parkland shootings. A common train of disagreement that tends to come from these conversations is the issue of regulation. Some people associated with a particular political leaning argue that more regulation leads to fewer guns which leads to less gun violence. Other people with the opposite political inclination argue that the regulations would only be followed by people who wouldn't be committing gun crimes anyway, and the regulations would just give wrongdoers more incentive to do wrong without the fear of an armed bystander. However, rather than presenting unbiased information to the public, news sources have decided to join one side of the argument or the other and sensationalize the issues to make more money. On the regulation-is-good side of the argument (henceforth referred to as pro-regulation), there is [this article](https://www.theatlantic.com/politics/archive/2015/08/the-states-with-the-most-gun-laws-see-the-fewest-gun-related-deaths/448044/) and [this more scholarly article from Stanford](https://news.stanford.edu/2017/12/07/new-study-analyzes-recent-gun-violence-research/). On the other, regulation-is-bad side of the argument (henceforth the anti-regulation stance) is [this article](http://insider.foxnews.com/2018/03/28/chicago-murder-rate-sky-high-despite-strict-gun-control-laws). Because, as mentioned before, news is sensationalist and we can't believe anything they say, we will do the data ourselves. We will be getting our information on gun crime from [this Kaggle data set](https://www.kaggle.com/jameslko/gun-violence-data) and cross referencing it with [this data set on gun laws](https://www.statefirearmlaws.org/table.html). We will play with weights based on issues that matter to us using [this supporting table](https://www.statefirearmlaws.org/glossary.html). Finally, we will adjust our gun crime rates for population using a [dataset](https://www.census.gov/data/datasets/2017/demo/popest/state-total.html) from the census bureau. After we store this data in R and make it pretty and clean, we will visualize it with multiple charts, perform some regression work, and even use ML to predict future gun violence rates. Of course, I don't know where this data came from exactly, it could come from people who are as bad as the news sources. Also, we must understand that there are several factors at work in an issue like this one, and just because a correlation was found doesn't mean there is causation.   

In terms of hypothesis testing, our null hypothesis is that there is no relation between gun violence and gun regulation, and our alternative hypothesis is that there is a relation between gun violence and gun regulation. More general hypothesis test information can be found [here](http://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/). 

As a final thought, I wrote the preceding without doing any data analysis, so I will be just as surprised by the results of these procedures when I finish running them as you will be when you read this for the first time!

## Getting Started


We will be doing this project in the language R in the RStudio. You can download RStudio [here](https://www.rstudio.com/products/rstudio/download/). If you want to get some more background information on R and RStudio, visit [this page](https://support.rstudio.com/hc/en-us/articles/201141096-Getting-Started-with-R), which has links to various resources for familiarizing yourself with R. I will try to be very thorough here too, however. 

Once you have R installed and have familiarized yourself with it to the extent you desire, sign up for a Kaggle account (I used my burner Google account and it was quite easy) and download the [Kaggle data set](https://www.kaggle.com/jameslko/gun-violence-data) and the [data set on gun laws](https://www.statefirearmlaws.org/table.html) as CSV files (the Kaggle will download as a zipped folder with a CSV in it, you have to choose CSV from a list of options for the other data). You can open these files in Microsoft Excell or another spreadsheet program to inspect the data or look at the raw information in a text editor to get a feel for what R will actually be seeing.  

Once you have RStudio open, got to File->New Project and create a directory for this project. This is what I will call the project's home directory, and all associated files for the project should be put in this folder. 


##Data Curation 

We will load in the CSV for the Kaggle gun violence data first. If you haven't already put the CSV in the project's home directory, do so. 

```{r load_violence}
#This library has everything and is fantastic, I've used it for 
#every R project I've done
library(tidyverse)

#Pulls the data form our CSV and stores it as R's 
#data structure, a data frame
violence_full <- read.csv("gun-violence-data_01-2013_03-2018.csv")

#this pipeline selects the first five colums and the
#first six rows of the dataset and prints. 
violence_full %>% select(1:5)%>%head()
```
As a note, all code will be commented with a max of two lines, and if I want to expound on my notes I will do it after the code block. 

For example, the last line is what is called a pipeline. The '%>%' operator takes the result of the thing in front of it and passes it as the first argument of the thing behind it. These pipes usually start with a dataset, in this case violence_full. Select picks columns, and this range means columns 1-5 inclusive. Head selects the first few rows of a dataset. If a line does nothing but create a value or mention a value without storing it, that value is printed. 

Also, if you have not already installed a library, go to the packages tab (by default in the bottom right pane in RStudio), click the "install" button, type in the name of the package you are looking for, and click "Install."

Now we will add the [data set on gun laws](https://www.statefirearmlaws.org/table.html) and the [census dataset](https://www2.census.gov/programs-surveys/popest/tables/2010-2017/state/totals/nst-est2017-01.xlsx). The government wasn't kind enough to provide us with a CSV, so we will have to do a slightly different procedure for the .xlsx file they do provide. 


```{r load_gunlaws_population}
#Pulls the data form our CSV and stores it as R's 
#data structure, a data frame
gunlaws_full <- read.csv("raw_data.csv")


#this pipeline selects the first five colums and the
#first six rows of the dataset and prints. 
gunlaws_full %>% select(1:5)%>%head()

#library for reading .xlsx
library(xlsx)

#the second argument in read.xlsx is the sheet index
#it can also be the sheet name
pops_full <- read.xlsx("nst-est2017-01.xlsx", 1)

pops_full %>% select(1:5)%>%head()
```

You will note that the dataframe for pops_full has a lot of junk in it. If you open it and the CSV for gun laws in Excell and compare them, you will notice that the .xlsx contains a lot more data than the gun laws CSV, and that most of this data will not be needed. We'll deal with that in a bit. 

If you want more information about reading in weird datafiles, check out [this link](https://www.statmethods.net/input/importingdata.html).  

If you look at the website for the [categories](https://www.statefirearmlaws.org/glossary.html), you'll notice that there is no option to download the data. We will have to [scrape](https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/) it. As outlined in that link, we will have to tell R what sort of element it is looking for to collect the data from. If you don't want to use the chrome extension the link talks about, you will have to inspect the element and select the class that that element is part of. You will put that in the argument of html_nodes. This will return a list of matches; you will have to select the one you want. In my case I wanted the first match it found. 

```{r scrape_cats}
#the library for scraping
library(rvest)

#read in webpage. 
#usually a web address
webpage <- read_html("State Firearm Laws - Use the Database Glossary Search Tool.html")

cats_full <- webpage %>%
  html_nodes(".table") %>%
  .[[1]] %>%
  html_table(fill = TRUE)

head(cats_full)
 
```

One snag I ran into here is that the page is dynamically loaded, so it only worked when I right clicked on the page and selected "Save page as..." and put the result in my project's home direcory. 


## Making Data Pretty

As mentioned before, there is a lot of junk in the pops_full dataset. We only want the data and done of the fluff. I'll remind you what this dataset looks like:

```{r show_popfull}
head(pops_full)
```

You'll notice that we only want a few of the columns (called atributes): the ones with the state names and the dates. You will also notice that there are some extra rows (called entities). The following code trims them down.

```{r trim_pop}
#library for text manipulation
library(stringr)

#the arrow is a storage operator
pops_trimmed <- pops_full %>%
  #removes attributes 2 and 3
  select(-(2:3)) %>%
  #removes first 8 entities
  slice(-(1:8)) %>%
  #removes last 6 entities
  slice(-(52:58) ) %>%
  #change the column names (temporarily for ease of use)
  #the ticks let us use code we wouldn't usually put in a pipeline
  `colnames<-`(c("State", "2010", "2011", "2012", "2013", 
                 "2014", "2015", "2016", "2017")) %>%
  #remove the first character in the string for State
  mutate(State = str_sub(State, 2, 25)) %>%
  #change 2010 (ticks because its a number) to numeric data
  transform(`2010` = as.double(as.character(`2010`))) %>%
  #change the column names (again...)
  #try without this to see what happens.
  `colnames<-`(c("State", "2010", "2011", "2012", "2013", 
                 "2014", "2015", "2016", "2017"))
  
  

head(pops_trimmed)

```

You will notice that I changed the state names to remove the periods, changed the attribute names, and changed the 2010 attribute to neumaric (helpful later, when we want them to be numbers: it was treating it as text). For more information on string manipulation, check [this link](http://r4ds.had.co.nz/strings.html). 

However, we're not done. We want to be able to join the violence dataset to this based on year and state eventually. This dataset as it stands is not very condusive to this, it would be better if there was a separate attribute for each state in each year. This also plays into the idea of [tidy data](https://vita.had.co.nz/papers/tidy-data.pdf) (incidentally, this seems to be the paper Professor Bravo used for his class notes--might want to cite this Professor). 

```{r tidy_pops}
#gather(dataset, what attribute names turns into, what the data turns into, 
# what not to include)
pops_tidy <- gather(pops_trimmed, year, population, -State)
head(pops_tidy)

```

Also, the violence_full dataset has more attributes than we need, and the date attribute is not seen by R as a date. To use it effectivly as a date, we would have to "date" attribute to a _date_ attribute. This involves some [fanciness](http://r4ds.had.co.nz/dates-and-times.html), and we really don't need the full date for this project, so we won't bother. We will create a year attribute for use when joining with the population data for analysis instead. 


```{r trim_violence}

violence_trimmed <- violence_full %>%
  #select the 4 attributes we want
  select(date, state, n_killed, n_injured, longitude, latitude) %>%
  mutate(date = str_sub(date, 1, 4)) 

head(violence_trimmed)
```

This dataset is really cool and I regret chopping out this much data. I am sure someone particularly motivated could do some really cool stuff with all the data this thing has. If you haven't looked more closely at it yet, do so!


## Distraction #1: Mapping Data
Because we are taking so long to get to our final data, and because we now have a hyper cool dataset with latitude and longitude, we are going to make cool maps! Hooray!  

One thing about this is, as a tangent, I will not be going into extreme detail about this section. 

Here, we will make an ultra-cool [layered](https://rstudio.github.io/leaflet/showhide.html) map, with a layer per year of data, showing the location of the reports of gun violence. 

```{r marked_map}
#the cool library for making maps
library(leaflet)

violence_map <- na.omit(violence_trimmed)

markermap <- leaflet(violence_map) %>%
  addTiles() %>%
  addCircleMarkers(~{as.numeric(unlist(violence_map%>%filter(date==2013)%>%select(longitude)))},
                    ~{as.numeric(unlist(violence_map%>%filter(date==2013)%>%select(latitude)))}, 
                    radius = 2, 
                    color = 'red',
                    group="2013")%>%
  addCircleMarkers(~{as.numeric(unlist(violence_map%>%filter(date==2014)%>%select(longitude)))},
                    ~{as.numeric(unlist(violence_map%>%filter(date==2014)%>%select(latitude)))}, 
                    radius = 2, 
                    color = 'orange',
                    group="2014") %>%
  addCircleMarkers(~{as.numeric(unlist(violence_map%>%filter(date==2015)%>%select(longitude)))},
                    ~{as.numeric(unlist(violence_map%>%filter(date==2015)%>%select(latitude)))}, 
                    radius = 2, 
                    color = 'green',
                    group="2015") %>%
  addCircleMarkers(~{as.numeric(unlist(violence_map%>%filter(date==2016)%>%select(longitude)))},
                    ~{as.numeric(unlist(violence_map%>%filter(date==2016)%>%select(latitude)))}, 
                    radius = 2, 
                    color = 'blue',
                    group="2016") %>%
  addCircleMarkers(~{as.numeric(unlist(violence_map%>%filter(date==2017)%>%select(longitude)))},
                    ~{as.numeric(unlist(violence_map%>%filter(date==2017)%>%select(latitude)))}, 
                    radius = 2, 
                    color = 'purple',
                    group="2017") %>%
  addCircleMarkers(~{as.numeric(unlist(violence_map%>%filter(date==2018)%>%select(longitude)))},
                    ~{as.numeric(unlist(violence_map%>%filter(date==2018)%>%select(latitude)))}, 
                    radius = 2, 
                    color = 'black',
                    group="2018") %>%
  addLayersControl(
    overlayGroups = c("2013", "2014", "2015", "2016", "2017", "2018"),
    options = layersControlOptions(collapsed = FALSE)
  ) %>% 
  hideGroup("2014") %>%
  hideGroup("2015") %>%
  hideGroup("2016") %>%
  hideGroup("2017") %>%
  hideGroup("2018") 

markermap

```

I know that this

You can find more about the leaflet [here](https://rstudio.github.io/leaflet/).  













