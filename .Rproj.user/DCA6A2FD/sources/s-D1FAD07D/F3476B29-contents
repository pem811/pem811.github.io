---
title: "Project 3.2"
author: "Paul Motter"
date: "May 4, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


**PART 2**
```{r setup_2, include=FALSE}
library(tidyverse)
library(lubridate)
library(ISLR)
library(cvTools)
library(tree)
library(randomForest)
library(caret)
library(ROCR)
theme_set(theme_bw())
```

```{r copied_1}

csv_file <- "Affordability_Wide_2017Q4_Public.csv"
tidy_afford <- read_csv(csv_file) %>%
  filter(Index == "Mortgage Affordability") %>%
  drop_na() %>%
  filter(RegionID != 0, RegionName != "United States") %>%
  dplyr::select(RegionID, RegionName, matches("^[1|2]")) %>%
  gather(time, affordability, matches("^[1|2]")) %>%
  type_convert(col_types=cols(time=col_date(format="%Y-%m")))
tidy_afford

tidy_afford %>%
  ggplot(aes(x=time,y=affordability,group=factor(RegionID))) +
  geom_line(color="GRAY", alpha=3/4, size=1/2) +
  labs(title="County-Level Mortgage Affordability over Time",
          x="Date", y="Mortgage Affordability")

```

```{r copied_2}
outcome_df <- tidy_afford %>%
  mutate(yq = quarter(time, with_year=TRUE)) %>%
  filter(yq %in% c("2016.4", "2017.4")) %>%
  select(RegionID, RegionName, yq, affordability) %>%
  spread(yq, affordability) %>%
  mutate(diff = `2017.4` - `2016.4`) %>%
  mutate(Direction = ifelse(diff>0, "up", "down")) %>%
  select(RegionID, RegionName, Direction)
outcome_df
```


```{r copied_3 }
predictor_df <- tidy_afford %>%
  filter(year(time) <= 2016)
```


The question I chose to answer: Is a random forest better than a decision tree?   
   
The reason I chose this is becuase I find both topics interesting. The idea of creating many trees with a random aspect and then averaging the results is pretty cool. In my AI class (421) we are always talking about ways to not get stuck in local minima, and this is an interesting way to avoid that potential problem. We also talk about decision trees in that class which led me to pick that method as well. Combined, this is an interesting problem as it can justify the use of extra work to make a random forest vs. a single tree.   
  
In the code below, I honestly copied a lot from the project description (it was there, alright?). I will explain it though.  
  
The first pipeline selects the data we really need from the dataframe and manipulates it so every area is an entitiy. This is needed to feed it into the predictors.  
  
  
Then, two matricies are produced that are offset (ex. matrix_1 does not include the first quarter). This way, when one is suptracted from the other, a matrix of differences in affordability will be produced. This is then joined back with the regionIDs and true directions from the 2017 data, which will be used to test the models' training against.    

```{r random_forest}
wide_df <- predictor_df %>%
  select(RegionID, time, affordability) %>%
  tidyr::spread(time, affordability)

matrix_1 <- wide_df %>%
  select(-RegionID) %>%
  as.matrix() %>%
  .[,-1]

matrix_2 <- wide_df %>%
  select(-RegionID) %>%
  as.matrix() %>%
  .[,-ncol(.)]

diff_df <- (matrix_1 - matrix_2) %>%
  magrittr::set_colnames(NULL) %>%
  as_data_frame() %>%
  mutate(RegionID = wide_df$RegionID)

final_df <- diff_df %>%
  inner_join(outcome_df %>% select(RegionID, Direction), by="RegionID") %>%
  mutate(Direction=factor(Direction, levels=c("down", "up")))

head(final_df)

```

I have added comments (and kept the ones I copied of course) in this code to explain it. Also ripped shamelessly from the examples (again, you gave it to me).



```{r bs2}
#end digits of an old phone number
set.seed(4352)


# create the cross-validation partition, k = # of folds
result_df <- createFolds(final_df$Direction, k=10) %>%
  # fit models and gather results. This will run 10 times, one for each fold
  purrr::imap(function(test_indices, fold_number) {
    # split into train and test for the fold
    train_df <- final_df %>%
      select(-RegionID) %>%
      slice(-test_indices)
    
    test_df <- final_df %>%
      select(-RegionID) %>%
      slice(test_indices)
  
    # fit the two models
    rf <- randomForest(Direction~., data=train_df)
    tr <- tree(Direction~., data=train_df)
    
    
    # gather results
    test_df %>%
      select(observed_label = Direction) %>%
      mutate(fold=fold_number) %>%
      mutate(prob_positive_rf = predict(rf, newdata=test_df, type="prob")[,"up"]) %>%
      # add predicted labels for rf using a 0.52 probability cutoff (seems to be a little better)
      mutate(predicted_label_rf = ifelse(prob_positive_rf > 0.52, "up", "down")) %>%
      #WHY CAN'T THEY STANDARDIZE THIS?
      mutate(prob_positive_tr = predict(tr, newdata=test_df)[,"up"]) %>%
      # add predicted labels for tr using a 0.5 probability cutoff 
      mutate(predicted_label_tr = ifelse(prob_positive_tr > 0.5, "up", "down")) 
}) %>%
  # combine the five result data frames into one
  purrr::reduce(bind_rows)
head(result_df)
```


```{r bs2_C1}
error_rates <- result_df %>%
  #calculate error
  mutate(error_rf = observed_label != predicted_label_rf,
         error_tr = observed_label != predicted_label_tr) %>%
  #gather and take a mean (true(is error) = 1, false = 0)
  group_by(fold)%>%
  summarize(rf = mean(error_rf), tr = mean(error_tr)) %>%
  tidyr::gather(model, error, -fold)

dotplot(error~model, data=error_rates, ylab="Mean Prediction Error")

error_rates  %>%
  lm(error~model, data=.) %>%
  broom::tidy()

```


```{r bs3_c}
# create a list of true observed labels 
labels <- split(result_df$observed_label, result_df$fold)

# now create a list of predictions for the RF and pass it to the ROCR::prediction function
predictions_rf <- split(result_df$prob_positive_rf, result_df$fold) %>% prediction(labels)


# do the same for the tree
predictions_tr <- split(result_df$prob_positive_tr, result_df$fold) %>% prediction(labels)

# compute average AUC for the RF
mean_auc_rf <- predictions_rf %>%
  performance(measure="auc") %>%
  # I know, this line is ugly, but that's how it is
  slot("y.values") %>% unlist() %>% 
  mean()

# compute average AUC for the tree
mean_auc_tr <- predictions_tr %>%
  performance(measure="auc") %>%
  slot("y.values") %>% unlist() %>% 
  mean()

# plot the ROC curve for the RF
predictions_rf %>%
  performance(measure="tpr", x.measure="fpr") %>%
  plot(avg="threshold", col="orange", lwd=2)

# plot the ROC curve for the tree
predictions_tr %>%
  performance(measure="tpr", x.measure="fpr") %>%
  plot(avg="threshold", col="blue", lwd=2, add=TRUE)

# add a legend to the plot
legend("bottomright",
       legend=paste(c("rf", "tree"), " AUC:", round(c(mean_auc_rf, mean_auc_tr), digits=3)),
       col=c("orange", "blue"))

```


When looking at the graph, it seems that the random forest has the opportunity to be more preciese, but also can produce a result with a high error rate. The tree predictor seems more consistant. Both methods seem to be very similar in effectivness. Indeed, when looking at the output of the regression, we see that the modeltr variable (0.08) is positive, but only slightly so. Because the intercept is the average error rate for the random forests, this positive value means that the tree's average error rate is that much higher. The p-value for the modeltr entry is not very close to zero, indicating that a single tree could not reasonably be called significantly worse (or different). Coincidentally, I had made an attempt at this before the extra notes for the second part of this project were released, and when a cutoff isn't defined for the random forest, it does significantly better than the tree. 

The areas under the curve are less than stellar. If the AUC = .5, that means that the method in question gets as many false positives as true ones: it is just as good as random. These values, both around .6, aren't much better than that. Therefore, these classifiers do not seem to be very effective.  
